{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24292,"status":"ok","timestamp":1644234383915,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"},"user_tz":-540},"id":"rDU5ezLW_M6m","outputId":"9beb3d77-0304-4838-ef0a-fb52dfc9955e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBuQsmJu_WTc"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","from IPython.display import clear_output\n","import itertools"]},{"cell_type":"markdown","metadata":{"id":"HUc1_W3WA7cy"},"source":["# Concat dataset - neg early & late"]},{"cell_type":"markdown","metadata":{"id":"iu33q8oyBGsq"},"source":["* negative early dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xqpj8XVfBNaz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644234416802,"user_tz":-540,"elapsed":32895,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"eaba5a54-6f43-4226-ff9c-2ac6bab87912"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has been merged\n","                                                text\n","0                                                 V1\n","1  ['presid', 'rodrigo', 'dutert', 'mass', 'test'...\n","2  ['omg', 'neighbour', 'told', 'cousin', 'work',...\n","3  ['pile', 'stadium', 'made', 'coronaviruspandem...\n","4                  ['well', 'laid', 'till', 'notic']\n"]}],"source":["directory = '/content/drive/MyDrive/covid analysis/negative_1'\n","neg_early = pd.DataFrame(columns=['text', 'score', 'index'])\n","\n","for file in os.listdir(directory):\n","    filename = os.fsdecode(file)\n","    data = pd.read_csv(directory + '/' +filename, header=None, names=['index', 'text', 'ave_sentiment'])\n","    neg_early = pd.concat([neg_early, data])\n","\n","print(\"Data has been merged\")\n","neg_early = neg_early.astype({'text': 'str'})\n","\n","neg_early = neg_early.drop(['index', 'score', 'ave_sentiment'], axis = 1)\n","print(neg_early.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNO_S1YAM5Pm"},"outputs":[],"source":["data = []\n","for i in range(len(neg_early)):\n","  if neg_early.iloc[i][0] != 'V1' :\n","    data.append(neg_early.iloc[i][0])\n","\n","neg_er = []\n","for i in data:\n","  doc = ''\n","  for j in i.split() :\n","    word = ''.join(filter(str.isalnum, j))\n","    doc += word + ' '\n","  neg_er.append(doc)"]},{"cell_type":"code","source":["!pip install pyLDAvis\n","clear_output()"],"metadata":{"id":"KQ9u7WgN_BGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","import gensim.corpora as corpora\n","from pprint import pprint\n","from gensim.models.coherencemodel import CoherenceModel\n","import pyLDAvis.gensim_models as gensimvis\n","import pyLDAvis\n","import warnings\n","import time\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel"],"metadata":{"id":"6WSV6Rkv_DLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg_er = np.array(neg_er)\n","ne = [d.split() for d in neg_er]\n","dict_ne = corpora.Dictionary(ne)\n","dtm_ne = [dict_ne.doc2bow(i) for i in ne]"],"metadata":{"id":"-MiIhx_j_GC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최적의 토픽 수 구하기\n","coherences_cv=[]\n","coherences_um=[]\n","perplexitiesT=[]\n","passes=[]\n","warnings.filterwarnings('ignore')\n","\n","for i in range(10):\n","    if i==0:\n","        ntopics = 2\n","    else:\n","        ntopics += 2\n","    #nwords = 100\n","    tic = time.time()\n","    lda4 = gensim.models.ldamodel.LdaModel(corpus = dtm_ne, id2word=dict_ne, num_topics=ntopics)\n","    print('ntopics',ntopics,time.time() - tic)\n","\n","    cm_cv = CoherenceModel(model=lda4, texts = ne, corpus = dtm_ne, dictionary = dict_ne, coherence='c_v')\n","    coherence_cv = cm_cv.get_coherence()\n","    print(\"Coherence_cv\",coherence_cv)\n","    coherences_cv.append(coherence_cv)\n","\n","    umass = CoherenceModel(model = lda4, corpus = dtm_ne, dictionary = dict_ne, coherence = 'u_mass')\n","    coherence_um = umass.get_coherence()\n","    print(\"Coherence_um\",coherence_um)\n","    coherences_um.append(coherence_um)\n","\n","    perpl = lda4.log_perplexity(dtm_ne)\n","    print('Perplexity: ', perpl,'\\n\\n')\n","    perplexitiesT.append(perpl)"],"metadata":{"id":"k9GuoZvX_oag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num = 16\n","LDA_ne = gensim.models.ldamodel.LdaModel(corpus = dtm_ne, id2word = dict_ne, num_topics = num)\n","pprint(LDA_ne.print_topics())\n","doc_ne = LDA_ne[dtm_ne]"],"metadata":{"id":"bH0LbKXE_O2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["warnings.filterwarnings('ignore')\n","\n","cm = CoherenceModel(model = LDA_ne, texts = ne, corpus = dtm_ne, dictionary = dict_ne, coherence='c_v')\n","coherence = cm.get_coherence()\n","print(\"Coherence : \", coherence)\n","\n","umass = CoherenceModel(model = LDA_ne, corpus = dtm_ne, dictionary = dict_ngl, coherence = 'u_mass')\n","coherence_um = umass.get_coherence()\n","print(\"Coherence : \", coherence_um)\n","\n","perplex = LDA_ne.log_perplexity(dtm_ne)\n","print(\"Perplexity : \", perplex)"],"metadata":{"id":"TFuOGe4f_X4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sT0a7C3UNbUe"},"source":["* negative late dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wATcGUyWNc9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644239318385,"user_tz":-540,"elapsed":17089,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"c2636f3e-3c9c-45c7-d886-dfe32fd2a026"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has been merged\n","                                                text\n","0                                                 V1\n","1  ['global', 'fight', 'research', 'world', 'acce...\n","2  ['montana', 'governor', 'action', 'remind', 'a...\n","3  ['delta', 'brutal', 'variant', 'decim', 'rural...\n","4  ['republican', 'refus', 'get', 'vaccin', 'wear...\n"]}],"source":["directory = '/content/drive/MyDrive/covid analysis/negative_2'\n","neg_late = pd.DataFrame(columns=['text', 'score', 'index'])\n","\n","for file in os.listdir(directory):\n","    filename = os.fsdecode(file)\n","    data = pd.read_csv(directory + '/' +filename, header=None, names=['index', 'text', 'ave_sentiment'])\n","    neg_late = pd.concat([neg_late, data])\n","\n","print(\"Data has been merged\")\n","neg_late = neg_late.astype({'text': 'str'})\n","\n","neg_late = neg_late.drop(['index', 'score', 'ave_sentiment'], axis = 1)\n","print(neg_late.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJ2KZ5z4Nj7S"},"outputs":[],"source":["data = []\n","for i in range(len(neg_late)):\n","  if neg_late.iloc[i][0] != 'V1' :\n","    data.append(neg_late.iloc[i][0])\n","neg_la = []\n","for i in data :\n","  doc = ''\n","  for j in i.split() :\n","    word = ''.join(filter(str.isalnum, j))\n","    doc += word + ' '\n","  neg_la.append(doc)"]},{"cell_type":"code","source":["neg_la = np.array(neg_la)\n","ngl = [d.split() for d in neg_la]\n","dict_ngl = corpora.Dictionary(ngl)\n","dtm_ngl = [dict_ngl.doc2bow(i) for i in ngl]"],"metadata":{"id":"o16q1U2MTjB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최적의 토픽 수 구하기\n","coherences_cv=[]\n","coherences_um=[]\n","perplexitiesT=[]\n","passes=[]\n","warnings.filterwarnings('ignore')\n","\n","for i in range(10):\n","    if i==0:\n","        ntopics = 2\n","    else:\n","        ntopics += 2\n","    #nwords = 100\n","    tic = time.time()\n","    lda4 = gensim.models.ldamodel.LdaModel(corpus = dtm_ngl, id2word=dict_ngl, num_topics=ntopics)\n","    print('ntopics',ntopics,time.time() - tic)\n","\n","    cm_cv = CoherenceModel(model=lda4, texts = ngl, corpus = dtm_ngl, dictionary = dict_ngl, coherence='c_v')\n","    coherence_cv = cm_cv.get_coherence()\n","    print(\"Coherence_cv\",coherence_cv)\n","    coherences_cv.append(coherence_cv)\n","\n","    umass = CoherenceModel(model = lda4, corpus = dtm_ngl, coherence = 'u_mass')\n","    coherence_um = umass.get_coherence()\n","    print(\"Coherence_um\",coherence_um)\n","    coherences_um.append(coherence_um)\n","\n","    perpl = lda4.log_perplexity(dtm_ngl)\n","    print('Perplexity: ', perpl,'\\n\\n')\n","    perplexitiesT.append(perpl)"],"metadata":{"id":"6im8bXw3ASeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IyjRyZomX5h"},"outputs":[],"source":["num = 6\n","LDA_ngl = gensim.models.ldamodel.LdaModel(corpus = dtm_ngl, id2word = dict_ngl, num_topics = num)\n","pprint(LDA_ngl.print_topics())\n","doc_ngl = LDA_ngl[dtm_ngl]"]},{"cell_type":"code","source":["pyldavis_html_path_ngl = 'negative_late_lda.html'\n","data_ngl = gensimvis.prepare(LDA_ngl, dtm_ngl, dict_ngl)\n","pyLDAvis.display(data_ngl)\n","pyLDAvis.save_html(data_ngl, pyldavis_html_path_ngl)"],"metadata":{"id":"kEBITeR2u990"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"topicModeling_negative.ipynb","provenance":[],"authorship_tag":"ABX9TyN9AFgBL5GBbKkd0GIQHk/g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}