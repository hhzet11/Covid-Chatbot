{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxYzRSdYkOx3ZLjFsJtEUA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# install the packages\n","\n"],"metadata":{"id":"ExuhQwASRUeI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTHEsTP0v_pS","executionInfo":{"status":"ok","timestamp":1643288442567,"user_tz":-540,"elapsed":15201,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"24125700-5960-4a4f-e47e-bf72e3cb500a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BudtTMfCuvmI"},"outputs":[],"source":["from IPython.display import clear_output\n","!pip install twarc #Twarc\n","!pip install tweepy # Tweepy 3.8.0\n","!pip install argparse #Argparse 3.2\n","!pip install xtract #Xtract 0.1 a3\n","!pip install wget #Wget 3.2\n","clear_output()"]},{"cell_type":"code","source":["import gzip\n","import shutil\n","import os\n","import wget\n","import csv\n","import linecache\n","from shutil import copyfile\n","import ipywidgets as widgets\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"LO-7k4u-uydD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import tweepy\n","from tweepy import OAuthHandler\n","\n","# Authenticate\n","CONSUMER_KEY = \"J9rvfuIoUYXukMaxd58lJw8ZQ\" #@param {type:\"string\"}\n","CONSUMER_SECRET_KEY = \"CYtX7T7cpIDkG69qxECXq3ObRUwxDVqeSRuwlA4ggVShlRhmtR\" #@param {type:\"string\"}\n","ACCESS_TOKEN_KEY = \"1476442025268752388-EEOH89okpXx1WJOVhewOqHYfG1XzFQ\" #@param {type:\"string\"}\n","ACCESS_TOKEN_SECRET_KEY = \"hHvdGJuuNerww38p1hUJBRLMUfRyBEjddjqyFXHFdI3yw\" #@param {type:\"string\"}\n","\n","#Creates a JSON Files with the API credentials\n","with open('api_keys.json', 'w') as outfile:\n","    json.dump({\n","    \"consumer_key\":CONSUMER_KEY,\n","    \"consumer_secret\":CONSUMER_SECRET_KEY,\n","    \"access_token\":ACCESS_TOKEN_KEY,\n","    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n","     }, outfile)\n"],"metadata":{"id":"6o2A5Ct5uzwh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import clear_output\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download(\"punkt\")\n","nltk.download('wordnet')\n","\n","!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py -O parse_json_lite.py\n","!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py -O fields.py\n","\n","!pip install emot --upgrade\n","!pip install emoji --upgrade\n","!pip install autocorrect\n","!pip install langdetect\n","\n","clear_output()"],"metadata":{"id":"VZH2x8t2vE_0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filter English"],"metadata":{"id":"IMHugJ5NRcEt"}},{"cell_type":"code","source":["# Filtering first\n","import pandas as pd\n","import numpy as np\n","import json\n","import sys\n","import re\n","import string\n","# This will load the fields list\n","import fields\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from langdetect import detect\n","from collections import Counter\n","\n","fieldsFilter = fields.fields\n","fileN = '/content/drive/MyDrive/covid analysis/hydrated_1/hydrated_tweets_short_0327.json'\n","data = []\n","\n","with open(fileN, 'r') as f:\n","    for line in f:\n","        data.append(json.loads(line))\n","\n","def detect_lang(text):\n","    try :    \n","      return detect(text)\n","    except :\n","      return np.nan\n","\n","tweet_df = pd.json_normalize(data)\n","# Cleaner solution in case some of the fields in the list are non existent and/or have typos\n","tweet_df = tweet_df.loc[:, tweet_df.columns.isin(fieldsFilter)]\n","\n","tw = pd.DataFrame(columns=[\"created_at\", \"text\", \"lang\"], index=range(0, len(tweet_df)))\n","for j in range(len(tweet_df['text'])):\n","    tw.loc[j]['created_at'] = tweet_df.loc[j]['created_at']\n","    tw.loc[j]['text'] = tweet_df.loc[j]['text']\n","    tw.loc[j]['lang'] = detect_lang(tw.loc[j]['text'])\n","print(tw.head())"],"metadata":{"id":"qrw6vrv92Tbz","colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"status":"error","timestamp":1642419841521,"user_tz":-540,"elapsed":12070,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"3870c8d8-ee55-4882-e93e-b12fa0bfb263"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-0e232ea00ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtweet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# Cleaner solution in case some of the fields in the list are non existent and/or have typos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtweet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfieldsFilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# TODO: handle record value which are lists, at least error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m#       reasonably\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_to_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36mnested_to_record\u001b[0;34m(ds, prefix, sep, level, max_level)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# only dicts gets recurse-flattened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# only at level>1 do we rename the rest of the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             if not isinstance(v, dict) or (\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mmax_level\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             ):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#Creates a new clean dataset with the specified language (if specified)\n","filtered_language = 'en'\n","cnt = 0\n","en_tw = pd.DataFrame(columns=[\"created_at\", \"text\"], index=range(0, len(tw)))\n","for i in range(len(tw['text'])):\n","  if tw.loc[i]['lang'] == filtered_language:\n","    en_tw.loc[cnt]['created_at'] = tw.loc[i]['created_at']\n","    en_tw.loc[cnt]['text'] = tw.loc[i]['text']\n","    cnt += 1\n","\n","en_tw = en_tw.dropna(axis=0)\n","print(en_tw.head())\n","print(len(en_tw))\n","with open(\"english-0326\"+\".json\",'w') as write_json:\n","  write_json.write(en_tw.to_json(orient = 'records', lines=True))"],"metadata":{"id":"p77TDfr8MTd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filter by keywords"],"metadata":{"id":"ZeMznYSaRiBc"}},{"cell_type":"code","source":["en_tw['text'] = en_tw['text'].str.lower()\n","filtered = pd.DataFrame(columns=[\"created_at\", \"text\"], index=range(0, 5000000))\n","\n","#filtering with keywords\n","keywords = ['covid19', 'covid-19', 'corona', 'coronavirus', 'virus', 'pandemic', 'outbreak', 'wuhan']\n","cnt = 0\n","for j in range(len(en_tw)):\n","  temp = en_tw.loc[j]['text']\n","  for i in keywords :\n","    if i in temp :\n","      filtered.loc[cnt]['created_at'] = en_tw.loc[j]['created_at']\n","      filtered.loc[cnt]['text'] = temp\n","      cnt += 1\n","    continue\n","    \n","filtered = filtered.dropna(axis=0)\n","filtered = filtered.drop_duplicates()\n","print(len(filtered))\n","\n","with open(\"filtered_0326\"+\".json\",'w') as write_json:\n","  write_json.write(filtered.to_json(orient = 'records', lines=True))"],"metadata":{"id":"lP9Z3uhewotl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"gxMFLxAF7x0K"}},{"cell_type":"code","source":["# Filtering first\n","import pandas as pd\n","import numpy as np\n","import json\n","import sys\n","import string\n","import re\n","import fields\n","from emot.emo_unicode import EMOJI_UNICODE, EMOTICONS_EMO\n","import emoji\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","from collections import Counter\n","from autocorrect import Speller\n","from textblob import TextBlob\n","try:\n","    import cPickle as pickle\n","except ImportError: \n","    import pickle"],"metadata":{"id":"SJSAAORkQ152"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fieldsFilter = fields.fields\n","fileN = '/content/drive/MyDrive/covid analysis/filtered_2/filtered_1005.json'\n","\n","data = []\n","\n","with open(fileN, 'r') as f:\n","    for line in f:\n","        data.append(json.loads(line))\n","\n","tweet_new = pd.json_normalize(data)\n","tweet_new = tweet_new.loc[:, tweet_new.columns.isin(fieldsFilter)]\n","tweet_new['index'] = tweet_new.index"],"metadata":{"id":"vlPD_KtlQ3A0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Treat Emoji and Emoticons"],"metadata":{"id":"r-1DyuYoR3qQ"}},{"cell_type":"code","source":["emoticon_file = '/content/drive/MyDrive/covid analysis/Emoticon_Dict.p'\n","emoji_file = '/content/drive/MyDrive/covid analysis/Emoji_Dict.p'\n","\n","with open(emoticon_file, 'rb') as fp:\n","    Emoticon_Dict = pickle.load(fp)\n","\n","with open(emoji_file, 'rb') as fp:\n","    Emoji_Dict = pickle.load(fp)\n","Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n","\n","def convert_emoticons(text):\n","    for emot in Emoticon_Dict:\n","        text = str(text)\n","        text = text.replace(emot, \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n","        return text\n","\n","def convert_emojis(text):\n","    for emot in Emoji_Dict:\n","        text = str(text)\n","        text = text.replace(emot, \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n","        return text\n","\n","def give_emoji_free_text(text):\n","    return emoji.get_emoji_regexp().sub(r'', text)\n","\n","\n","# lowercasing\n","tweet_new['text'] = tweet_new['text'].str.lower()\n","\n","# convert emoji and emoticons\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : convert_emoticons(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : convert_emojis(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : give_emoji_free_text(x))\n"],"metadata":{"id":"M1x1woYqSytb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(tweet_new['text']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-K-UBI_wixF","executionInfo":{"status":"ok","timestamp":1643302806307,"user_tz":-540,"elapsed":48,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"61ad2677-5df9-44dc-fcc9-2ade23ec42ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["48317\n"]}]},{"cell_type":"markdown","source":["* Remove noise"],"metadata":{"id":"635Cnb1HR-UA"}},{"cell_type":"code","source":["def remove_urls(text):\n","    result = re.sub(r\"http\\S+\", \"\", text)\n","    return(result)\n","\n","def remove_twitter_urls(text):\n","    clean = re.sub(r\"pic.twitter\\S+\", \"\",text)\n","    return(clean)\n","\n","def remove_mentions(text):\n","    mentions = re.compile(r\"@\\w+ ?\")\n","    return mentions.sub(r\"\", text)\n","\n","def remove_numbers(text):\n","    return re.sub(r\"\\d+\", \"\", text)\n","\n","def remove_others(text) :\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)"],"metadata":{"id":"P8ikyy6bvHVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove noise\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_urls(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_twitter_urls(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_mentions(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_numbers(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_others(x))\n","print(len(tweet_new['text']))"],"metadata":{"id":"qp2__wyYRd7p","executionInfo":{"status":"ok","timestamp":1643302806723,"user_tz":-540,"elapsed":450,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e101bbec-8a8b-4d5c-920c-e1e6df048ac4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["48317\n"]}]},{"cell_type":"markdown","source":["* For sentimentr - score sentiment"],"metadata":{"id":"d20_MbkS-1t3"}},{"cell_type":"code","source":["punc_list = string.punctuation + '…' + '—' + '“' + '”' + '‘' + '’' + '•' + '↓' \n","def remove_punctuations(text):\n","    translator = str.maketrans(\"\", \"\", punc_list)\n","    return str(text).translate(translator)\n","\n","def non_english(text) :\n","    text = re.sub(r'\\W+', ' ', text)\n","    text = text.replace(\"[^a-zA-Z]\", \" \")\n","    word_tokens = word_tokenize(text)\n","    filtered_word = [w for w in word_tokens if all(ord(c) < 128 for c in w)]\n","    filtered_word = [w + \" \" for w in filtered_word]\n","    return \"\".join(filtered_word)\n","\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_punctuations(x))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : non_english(x))\n","print(len(tweet_new['text']))"],"metadata":{"id":"Vywgnfe8rNR9","executionInfo":{"status":"ok","timestamp":1643302815095,"user_tz":-540,"elapsed":8378,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43866be9-d0b6-4a81-ad51-3f3555ac347f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["48317\n"]}]},{"cell_type":"code","source":["tweet_new.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LqcdtFksk5_","executionInfo":{"status":"ok","timestamp":1643302815097,"user_tz":-540,"elapsed":17,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"84add82f-1369-4985-f99f-47634e94bb0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7a230bc6-bd4f-42f4-b0ac-7a9cb282b34e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>created_at</th>\n","      <th>text</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Tue Oct 05 04:01:49 +0000 2021</td>\n","      <td>india records new covid cases in past hours lo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tue Oct 05 04:01:48 +0000 2021</td>\n","      <td>jann arden says while covid is very real it wo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tue Oct 05 04:01:50 +0000 2021</td>\n","      <td>jann arden says while covid is very real it wo...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Tue Oct 05 04:02:17 +0000 2021</td>\n","      <td>pdf download and the people stayed home family...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Tue Oct 05 04:02:13 +0000 2021</td>\n","      <td>trying to understand why usa got hit so hard d...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a230bc6-bd4f-42f4-b0ac-7a9cb282b34e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7a230bc6-bd4f-42f4-b0ac-7a9cb282b34e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7a230bc6-bd4f-42f4-b0ac-7a9cb282b34e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                       created_at  ... index\n","0  Tue Oct 05 04:01:49 +0000 2021  ...     0\n","1  Tue Oct 05 04:01:48 +0000 2021  ...     1\n","2  Tue Oct 05 04:01:50 +0000 2021  ...     2\n","3  Tue Oct 05 04:02:17 +0000 2021  ...     3\n","4  Tue Oct 05 04:02:13 +0000 2021  ...     4\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":431}]},{"cell_type":"code","source":["#tweet_new = tweet_new.drop(['created_at'], axis = 'columns')\n","tweet_new.to_csv('/content/drive/MyDrive/covid analysis/sentence_2/1005.csv', columns = ['text', 'index'], header = True, index = False)"],"metadata":{"id":"-6I9dD3rWU6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Proceed NLP tasks"],"metadata":{"id":"khQmy6QuSAA1"}},{"cell_type":"code","source":["all_stopwords = stopwords.words('english')\n","all_stopwords.extend(['via', 'amp', 'due', 'one', 'nt'])\n","print(all_stopwords)\n","def stop_words(text):\n","    text_tokens = word_tokenize(text)\n","    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n","    return \" \".join(tokens_without_sw)\n","\n","punc_list = string.punctuation + '…' + '—' + '“' + '”' + '‘' + '’' + '•' + '↓' \n","def remove_punctuations(text):\n","    translator = str.maketrans(\"\", \"\", punc_list)\n","    return str(text).translate(translator)\n","\n","def non_english(text) :\n","    text = re.sub(r'\\W+', ' ', text)\n","    text = text.replace(\"[^a-zA-Z]\", \" \")\n","    word_tokens = word_tokenize(text)\n","    filtered_word = [w for w in word_tokens if all(ord(c) < 128 for c in w)]\n","    filtered_word = [w + \" \" for w in filtered_word]\n","    return \"\".join(filtered_word)\n","\n","keywords = ['covid', 'corona', 'coronavirus', 'virus', 'pandemic', 'outbreak', 'wuhan']\n","def remove_keywords(text):\n","    text_tokens = word_tokenize(text)\n","    tokens_without_sw = [word for word in text_tokens if not word in keywords]\n","    return tokens_without_sw\n","\n","def remove_onewords(text):\n","    correct = []\n","    for word in text :\n","      if len(word) >= 2 :\n","        correct.append(word)\n","    return correct\n","\n","ps = PorterStemmer()\n","def stem_text(text):\n","  stem = []\n","  for word in text :\n","    stem.append(ps.stem(word))\n","  return stem \n","\n","lemmatizer = WordNetLemmatizer()\n","def lem_text(text):\n","  lemm = []\n","  for word in text :\n","    lemm.append(lemmatizer.lemmatize(word))\n","  return lemm"],"metadata":{"id":"YwC2slZHRahW","executionInfo":{"status":"ok","timestamp":1643302815549,"user_tz":-540,"elapsed":465,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"17b1e63d-f7e5-4bc2-d6c4-8e402cc83fca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'via', 'amp', 'due', 'one', 'nt']\n"]}]},{"cell_type":"code","source":["print(tweet_new['text'].head(10))\n","print(tweet_new['text'].tail(10))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : stop_words(x))\n","print(len(tweet_new['text']))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_keywords(x))\n","print(len(tweet_new['text']))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : remove_onewords(x))\n","print(len(tweet_new['text']))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : stem_text(x))\n","print(len(tweet_new['text']))\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : lem_text(x))\n","print(tweet_new['text'].head(10))\n","print(tweet_new['text'].tail(10))\n","print(len(tweet_new['text']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EbS8cTYSjVEq","executionInfo":{"status":"ok","timestamp":1643302842026,"user_tz":-540,"elapsed":26484,"user":{"displayName":"이하영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16656722516517244964"}},"outputId":"a4dfde65-0ed2-4e56-932c-d020ce49deca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0    india records new covid cases in past hours lo...\n","1    jann arden says while covid is very real it wo...\n","2    jann arden says while covid is very real it wo...\n","3    pdf download and the people stayed home family...\n","4    trying to understand why usa got hit so hard d...\n","5    time is running out for schools to apply for f...\n","6    its a pandemic of the unvaccinated right how a...\n","7    since the start of the pandemic americans have...\n","8    covid cases among children are rising signific...\n","9    have you received your covid vaccination us ci...\n","Name: text, dtype: object\n","48307    covid india reports new cases in the last hour...\n","48308    potential for false positive results with cert...\n","48309    south africa launches digital covid vaccinatio...\n","48310    what an idiot i love the astros but they wont ...\n","48311    remember all vaccines are effective in prevent...\n","48312    the big question is is your government health ...\n","48313    since the start of the pandemic americans have...\n","48314    covid precuation in igbo from the rotary club ...\n","48315    help slow the spread of covid and identify at ...\n","48316                                                None \n","Name: text, dtype: object\n","48317\n","48317\n","48317\n","48317\n","0    [india, record, new, case, past, hour, lowest,...\n","1    [jann, arden, say, real, wont, ever, exist, tv...\n","2    [jann, arden, say, real, wont, ever, exist, tv...\n","3    [pdf, download, peopl, stay, home, famili, boo...\n","4    [tri, understand, usa, got, hit, hard, se, asi...\n","5    [time, run, school, appli, feder, fund, help, ...\n","6    [unvaccin, right, cant, know, dont, ask, right...\n","7        [sinc, start, american, die, death, worldwid]\n","8    [case, among, child, rise, significantli, fast...\n","9    [receiv, vaccin, u, citizen, older, receiv, mu...\n","Name: text, dtype: object\n","48307    [india, report, new, case, last, hour, activ, ...\n","48308    [potenti, fals, posit, result, certain, lot, e...\n","48309    [south, africa, launch, digit, vaccin, certif,...\n","48310                      [idiot, love, astro, wont, win]\n","48311    [rememb, vaccin, effect, prevent, sever, disea...\n","48312    [big, question, govern, health, offici, allow,...\n","48313        [sinc, start, american, die, death, worldwid]\n","48314                    [precuat, igbo, rotari, club, vi]\n","48315    [help, slow, spread, identifi, risk, case, soo...\n","48316                                               [none]\n","Name: text, dtype: object\n","48317\n"]}]},{"cell_type":"code","source":["#tweet_new = tweet_new.drop(['created_at'], axis = 'columns')\n","tweet_new.to_csv('/content/drive/MyDrive/covid analysis/preprocessed_2/1005.csv', columns = ['text', 'index'], header = True, index = False)"],"metadata":{"id":"X7OkLlG6sOQf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compute uni and n grams"],"metadata":{"id":"v6s59GiocB8m"}},{"cell_type":"code","source":["import csv \n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n","\n","def concat(text):\n","  return \" \".join([word for word in str(text).split()])\n","\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : concat(x))\n","\n","# Getting ngrams  \n","vectorizer = CountVectorizer(ngram_range = (1,1)).fit(tweet_new[\"text\"]) \n","\n","bag_of_words = vectorizer.transform(tweet_new[\"text\"])\n","sum_words = bag_of_words.sum(axis=0) \n","words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n","words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","\n","with open(\"1005-1gram.csv\", \"w\") as f:\n","    w = csv.writer(f)\n","    w.writerows(words_freq)"],"metadata":{"id":"Hh2msv04QAIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv \n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n","\n","def concat(text):\n","  return \" \".join([word for word in str(text).split()])\n","\n","tweet_new['text'] = tweet_new['text'].apply(lambda x : concat(x))\n","\n","# Getting ngrams  \n","vectorizer = CountVectorizer(ngram_range = (2,3)).fit(tweet_new[\"text\"]) \n","\n","bag_of_words = vectorizer.transform(tweet_new[\"text\"])\n","sum_words = bag_of_words.sum(axis=0) \n","words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n","words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","\n","with open(\"1005-ngram.csv\", \"w\") as f:\n","    w = csv.writer(f)\n","    w.writerows(words_freq)"],"metadata":{"id":"miYGtOsuR97C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KNMtUJUF0ZUl"},"execution_count":null,"outputs":[]}]}